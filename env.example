# Loosh Inference Validator Configuration
# Copy this file to .env and update with your values

# =============================================================================
# Network Configuration
# =============================================================================
NETUID=78
SUBTENSOR_NETWORK=finney
SUBTENSOR_ADDRESS=wss://entrypoint-finney.opentensor.ai:443

# Network Configuration - testnet
#NETUID=78
#SUBTENSOR_NETWORK=test
#SUBTENSOR_ADDRESS=wss://test.finney.opentensor.ai:443

# =============================================================================
# Wallet Configuration
# =============================================================================
# Note: Fiber only supports wallets in ~/.bittensor/wallets
WALLET_NAME=validator
HOTKEY_NAME=validator

# =============================================================================
# Miner Selection Parameters
# =============================================================================
MIN_MINERS=3
MAX_MINERS=10
MIN_STAKE_THRESHOLD=100

# =============================================================================
# Challenge Parameters (in seconds)
# =============================================================================
# Testnet: 10 seconds
# Mainnet: 300 seconds
CHALLENGE_INTERVAL_SECONDS=300
CHALLENGE_TIMEOUT_SECONDS=120
EVALUATION_TIMEOUT_SECONDS=300

# =============================================================================
# Scoring Parameters
# =============================================================================
SCORE_THRESHOLD=0.7

# =============================================================================
# Weights Update Interval
# =============================================================================
WEIGHTS_INTERVAL_SECONDS=1800

# =============================================================================
# Database Configuration
# =============================================================================
DB_PATH=validator.db
USERS_DB_PATH=users.db

# =============================================================================
# API Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Challenge API Configuration
# =============================================================================
CHALLENGE_API_URL=http://localhost:8080

# API key for Challenge API (fallback authentication)
# Primary authentication uses Fiber MLTS encryption.
# API key is used as fallback if Fiber handshake fails.
CHALLENGE_API_KEY=your-api-key

# Optional: API key for authenticating incoming challenge push requests
CHALLENGE_PUSH_API_KEY=

# =============================================================================
# Evaluation Configuration
# =============================================================================
# Deprecated: now uses CHALLENGE_API_URL
HEATMAP_UPLOAD_URL=http://localhost:8080/heatmap/upload

# =============================================================================
# LLM Configuration (for narrative generation - NOT CURRENTLY IN USE)
# =============================================================================
# Note: These settings are only used when ENABLE_NARRATIVE_GENERATION=true
# For production deployments with ENABLE_NARRATIVE_GENERATION=false, these are not needed
#
# IMPORTANT: The API endpoint must implement the OpenAI Chat Completions API format
# (see https://platform.openai.com/docs/api-reference/chat/create)
# The API interface must be compatible, but the underlying model does NOT need to be an OpenAI model.
# You can use any model (Llama, Qwen, Mistral, etc.) as long as the API follows OpenAI's format.
#
# Examples of compatible endpoints:
#   - OpenAI API (https://api.openai.com/v1/chat/completions)
#   - Azure OpenAI (https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions)
#   - vLLM (http://localhost:8000/v1/chat/completions)
#   - Ollama with OpenAI compatibility (http://localhost:11434/v1/chat/completions)
#   - Any endpoint that implements the OpenAI Chat Completions API format
#LLM_API_URL=
#LLM_API_KEY=
#LLM_MODEL=

# =============================================================================
# LLM Configuration (for narrative generation - NOT CURRENTLY IN USE)
# =============================================================================
# Note: These settings are only used when ENABLE_NARRATIVE_GENERATION=true
# For production deployments with ENABLE_NARRATIVE_GENERATION=false, these are not needed
#DEFAULT_MODEL=mistralai/Mistral-7B-v0.1
#DEFAULT_MAX_TOKENS=512
#DEFAULT_TEMPERATURE=0.7
#DEFAULT_TOP_P=0.95

# =============================================================================
# Sentence Transformer Configuration (for embeddings - REQUIRED)
# =============================================================================
# Sentence Transformer model used for generating embeddings during evaluation
# This model is ALWAYS required as it's used for heatmap generation and consensus scoring
# Default: sentence-transformers/all-MiniLM-L6-v2 (fast, efficient, good for semantic similarity)
# Alternative options:
#   - sentence-transformers/all-mpnet-base-v2 (more accurate but slower)
#   - sentence-transformers/paraphrase-MiniLM-L6-v2 (optimized for paraphrase detection)
SENTENCE_TRANSFORMER_MODEL=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO
LOG_FILE=

# =============================================================================
# Test Mode
# =============================================================================
# Enable test mode - picks first response without evaluation or heatmap generation
TEST_MODE=false

# =============================================================================
# Narrative Generation Configuration (OPTIONAL - Not required for operation)
# =============================================================================
# IMPORTANT: For production deployments, keep this set to false
# The validator does NOT need inference to function properly
# Enable narrative generation using LLM. When disabled, evaluation and heatmap
# generation still run, but narrative is skipped (saves LLM API costs and resources).
# Enable only for development/debugging to generate narrative summaries.
# Default: false
ENABLE_NARRATIVE_GENERATION=false

# =============================================================================
# Heatmap Generation Configuration
# =============================================================================
# Enable heatmap generation for consensus evaluation. When disabled, heatmaps
# will not be generated or uploaded.
# Default: true
ENABLE_HEATMAP_GENERATION=true

# =============================================================================
# Quality Plot Generation Configuration
# =============================================================================
# Enable quality plot generation (response length distribution). When enabled,
# generates quality plots alongside heatmaps when quality filtering is active.
# Quality plots show the distribution of response word counts and help visualize
# the quality filtering process.
# Default: false
ENABLE_QUALITY_PLOTS=false

# =============================================================================
# Challenge Mode Configuration
# =============================================================================
# Note: Only push mode is supported. Challenges are submitted via POST /challenges endpoint
# CHALLENGE_MODE=push  # Default and only supported mode

# =============================================================================
# Concurrency Configuration
# =============================================================================
# Maximum number of challenges to process concurrently
MAX_CONCURRENT_CHALLENGES=10

# =============================================================================
# Fiber MLTS Configuration
# =============================================================================
# Time-to-live for Fiber symmetric keys in seconds (default: 1 hour)
FIBER_KEY_TTL_SECONDS=3600
# Timeout for Fiber handshake operations in seconds
FIBER_HANDSHAKE_TIMEOUT_SECONDS=30
# Enable automatic key rotation for Fiber symmetric keys
FIBER_ENABLE_KEY_ROTATION=true

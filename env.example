# Loosh Inference Validator Configuration
# Copy this file to .env and update with your values

# =============================================================================
# Network Configuration
# =============================================================================
# NETUID: The subnet UID to connect to (78 for Loosh subnet on mainnet)
NETUID=78

# SUBTENSOR_NETWORK: Network name (finney=mainnet, test=testnet, local=local)
SUBTENSOR_NETWORK=finney

# SUBTENSOR_ADDRESS: Custom chain endpoint (RPC URL)
# You can specify your own subtensor node endpoint here
# Examples:
#   - Official Finney: wss://entrypoint-finney.opentensor.ai:443
#   - Custom node: ws://your_endpoint.your.network:9900
#   - Local node: ws://127.0.0.1:9944
SUBTENSOR_ADDRESS=wss://entrypoint-finney.opentensor.ai:443

# Network Configuration - testnet example
#NETUID=78
#SUBTENSOR_NETWORK=test
#SUBTENSOR_ADDRESS=wss://test.finney.opentensor.ai:443

# Network Configuration - custom endpoint example
#NETUID=78
#SUBTENSOR_NETWORK=finney
#SUBTENSOR_ADDRESS=ws://your_endpoint.your.network:9900

# =============================================================================
# Wallet Configuration
# =============================================================================
# Note: Fiber only supports wallets in ~/.bittensor/wallets
WALLET_NAME=validator
HOTKEY_NAME=validator

# =============================================================================
# Miner Selection Parameters
# =============================================================================
MIN_MINERS=3
MAX_MINERS=10
MIN_STAKE_THRESHOLD=100

# =============================================================================
# Challenge Parameters (in seconds)
# =============================================================================
# Testnet: 10 seconds
# Mainnet: 300 seconds
CHALLENGE_INTERVAL_SECONDS=300
CHALLENGE_TIMEOUT_SECONDS=120
EVALUATION_TIMEOUT_SECONDS=300

# =============================================================================
# Scoring Parameters
# =============================================================================
SCORE_THRESHOLD=0.7

# =============================================================================
# Weights Update Interval
# =============================================================================
WEIGHTS_INTERVAL_SECONDS=1800

# =============================================================================
# Database Configuration
# =============================================================================
DB_PATH=validator.db
USERS_DB_PATH=users.db

# =============================================================================
# API Configuration
# =============================================================================
# REST API configuration for the validator API server
# IMPORTANT: This port MUST be publicly accessible for the Challenge API to push challenges
# The Challenge API sends encrypted challenges to POST /fiber/challenge on this API
# Note: you may need to change this to 127.0.0.1 depending on your deployment model. 
# Docker doesn't like 127.0.0.1 and so 0.0.0.0 is preferred in that case.
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Axon Configuration (Bittensor Node Communication)
# =============================================================================
# Axon is the bittensor node's internal communication endpoint
# This is separate from the REST API above
#
# IMPORTANT: The axon port does NOT need public exposure
# Validators are clients that query miners - miners expose their axons, not validators
# Only change these if you have port conflicts or specific networking requirements
#
# AXON_IP: Internal IP address for the axon to bind to
# AXON_PORT: Internal port for the axon to bind to
# AXON_EXTERNAL_IP: External IP to advertise to the network (optional, auto-detected if not set)
# AXON_EXTERNAL_PORT: External port to advertise to the network (optional, uses AXON_PORT if not set)
# AXON_MAX_WORKERS: Maximum worker threads for handling axon requests
# AXON_TIMEOUT: Timeout for axon operations in seconds
AXON_IP=127.0.0.1
AXON_PORT=8099
#AXON_EXTERNAL_IP=
#AXON_EXTERNAL_PORT=
AXON_MAX_WORKERS=5
AXON_TIMEOUT=60

# =============================================================================
# Challenge API Configuration
# =============================================================================
CHALLENGE_API_URL=http://localhost:8080

# API key for Challenge API (fallback authentication)
# Primary authentication uses Fiber MLTS encryption.
# API key is used as fallback if Fiber handshake fails.
CHALLENGE_API_KEY=your-api-key

# Optional: API key for authenticating incoming challenge push requests
CHALLENGE_PUSH_API_KEY=

# =============================================================================
# Evaluation Configuration
# =============================================================================
# Deprecated: now uses CHALLENGE_API_URL
HEATMAP_UPLOAD_URL=http://localhost:8080/heatmap/upload

# =============================================================================
# LLM Configuration (for narrative generation - NOT CURRENTLY IN USE)
# =============================================================================
# Note: These settings are only used when ENABLE_NARRATIVE_GENERATION=true
# For production deployments with ENABLE_NARRATIVE_GENERATION=false, these are not needed
#
# IMPORTANT: The API endpoint must implement the OpenAI Chat Completions API format
# (see https://platform.openai.com/docs/api-reference/chat/create)
# The API interface must be compatible, but the underlying model does NOT need to be an OpenAI model.
# You can use any model (Llama, Qwen, Mistral, etc.) as long as the API follows OpenAI's format.
#
# Examples of compatible endpoints:
#   - OpenAI API (https://api.openai.com/v1/chat/completions)
#   - Azure OpenAI (https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions)
#   - vLLM (http://localhost:8000/v1/chat/completions)
#   - Ollama with OpenAI compatibility (http://localhost:11434/v1/chat/completions)
#   - Any endpoint that implements the OpenAI Chat Completions API format
#LLM_API_URL=
#LLM_API_KEY=
#LLM_MODEL=

# =============================================================================
# LLM Configuration (for narrative generation - NOT CURRENTLY IN USE)
# =============================================================================
# Note: These settings are only used when ENABLE_NARRATIVE_GENERATION=true
# For production deployments with ENABLE_NARRATIVE_GENERATION=false, these are not needed
#DEFAULT_MODEL=mistralai/Mistral-7B-v0.1
#DEFAULT_MAX_TOKENS=512
#DEFAULT_TEMPERATURE=0.7
#DEFAULT_TOP_P=0.95

# =============================================================================
# Sentence Transformer Configuration (for embeddings - REQUIRED)
# =============================================================================
# Sentence Transformer model used for generating embeddings during evaluation
# This model is ALWAYS required as it's used for heatmap generation and consensus scoring
# Default: sentence-transformers/all-MiniLM-L6-v2 (fast, efficient, good for semantic similarity)
SENTENCE_TRANSFORMER_MODEL=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO
LOG_FILE=

# =============================================================================
# Test Mode
# =============================================================================
# Enable test mode - picks first response without evaluation or heatmap generation
TEST_MODE=false

# =============================================================================
# Narrative Generation Configuration (OPTIONAL - Not required for operation)
# =============================================================================
# IMPORTANT: For production deployments, keep this set to false
# The validator does NOT need inference to function properly
# Enable narrative generation using LLM. When disabled, evaluation and heatmap
# generation still run, but narrative is skipped (saves LLM API costs and resources).
# Enable only for development/debugging to generate narrative summaries.
# Default: false
ENABLE_NARRATIVE_GENERATION=false

# =============================================================================
# Heatmap Generation Configuration
# =============================================================================
# Enable heatmap generation for consensus evaluation. When disabled, heatmaps
# will not be generated or uploaded.
# Default: true
ENABLE_HEATMAP_GENERATION=true

# =============================================================================
# Quality Plot Generation Configuration
# =============================================================================
# Enable quality plot generation (response length distribution). When enabled,
# generates quality plots alongside heatmaps when quality filtering is active.
# Quality plots show the distribution of response word counts and help visualize
# the quality filtering process.
# Default: false
ENABLE_QUALITY_PLOTS=false

# =============================================================================
# Challenge Mode Configuration
# =============================================================================
# Note: Only push mode is supported. Challenges are submitted via POST /challenges endpoint
# CHALLENGE_MODE=push  # Default and only supported mode

# =============================================================================
# Concurrency Configuration
# =============================================================================
# Maximum number of challenges to process concurrently
MAX_CONCURRENT_CHALLENGES=10

# =============================================================================
# Fiber MLTS Configuration
# =============================================================================
# Time-to-live for Fiber symmetric keys in seconds (default: 1 hour)
FIBER_KEY_TTL_SECONDS=3600
# Timeout for Fiber handshake operations in seconds
FIBER_HANDSHAKE_TIMEOUT_SECONDS=30
# Enable automatic key rotation for Fiber symmetric keys
FIBER_ENABLE_KEY_ROTATION=true

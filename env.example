# Loosh Inference Validator Configuration
# Copy this file to .env and update with your values
#
# NOTE: Operational parameters (miner selection, challenge timing, scoring,
# weight setting, concurrency, embedding model, Fiber MLTS, etc.) are
# hard-coded in validator/internal_config.py for network consistency.
# Only deployment-specific settings belong here.

# =============================================================================
# Network Configuration
# =============================================================================
NETUID=78
SUBTENSOR_NETWORK=finney
SUBTENSOR_ADDRESS=wss://entrypoint-finney.opentensor.ai:443

# Network Configuration - testnet
#NETUID=78
#SUBTENSOR_NETWORK=test
#SUBTENSOR_ADDRESS=wss://test.finney.opentensor.ai:443

# =============================================================================
# Wallet Configuration
# =============================================================================
# Note: Fiber only supports wallets in ~/.bittensor/wallets
WALLET_NAME=loosh_validator
HOTKEY_NAME=validator_hotkey

# =============================================================================
# Database Configuration
# =============================================================================
DB_PATH=/workspace/validator.db
USERS_DB_PATH=/workspace/users.db

# =============================================================================
# API Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8100

# =============================================================================
# Challenge API Configuration
# =============================================================================
CHALLENGE_API_URL=https://challenge.loosh.ai
CHALLENGE_API_KEY=your-challenge-api-key-here

# =============================================================================
# LLM Configuration (for narrative generation - NOT CURRENTLY IN USE)
# =============================================================================
# Note: These settings are only used when ENABLE_NARRATIVE_GENERATION=true
# (controlled in internal_config.py, default: false).
# For production deployments this is not needed.
#
# IMPORTANT: The API endpoint must implement the OpenAI Chat Completions API format
# (see https://platform.openai.com/docs/api-reference/chat/create)
# The API interface must be compatible, but the underlying model does NOT need to be
# an OpenAI model. You can use any model (Llama, Qwen, Mistral, etc.) as long as
# the API follows OpenAI's format.
#
# Examples of compatible endpoints:
#   - OpenAI API (https://api.openai.com/v1/chat/completions)
#   - vLLM (http://localhost:8000/v1/chat/completions)
#   - Ollama with OpenAI compatibility (http://localhost:11434/v1/chat/completions)
#LLM_API_URL=
#LLM_API_KEY=
#LLM_MODEL=

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO
#LOG_FILE=

# =============================================================================
# Test Mode
# =============================================================================
# Enable test mode - picks first response without evaluation or heatmap generation
TEST_MODE=false

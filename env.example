# Loosh Inference Validator Configuration
# Copy this file to .env and update with your values

# =============================================================================
# Network Configuration
# =============================================================================
NETUID=78
SUBTENSOR_NETWORK=finney
SUBTENSOR_ADDRESS=wss://entrypoint-finney.opentensor.ai:443

# Network Configuration - testnet
#NETUID=78
#SUBTENSOR_NETWORK=test
#SUBTENSOR_ADDRESS=wss://test.finney.opentensor.ai:443

# =============================================================================
# Wallet Configuration
# =============================================================================
# Note: Fiber only supports wallets in ~/.bittensor/wallets
WALLET_NAME=validator
HOTKEY_NAME=validator

# =============================================================================
# Miner Selection Parameters
# =============================================================================
MIN_MINERS=3
MAX_MINERS=10
MIN_STAKE_THRESHOLD=100

# =============================================================================
# Challenge Parameters (in seconds)
# =============================================================================
# Testnet: 10 seconds
# Mainnet: 300 seconds
CHALLENGE_INTERVAL_SECONDS=300
CHALLENGE_TIMEOUT_SECONDS=120
EVALUATION_TIMEOUT_SECONDS=300

# =============================================================================
# Scoring Parameters
# =============================================================================
SCORE_THRESHOLD=0.7

# =============================================================================
# Weights Update Interval
# =============================================================================
WEIGHTS_INTERVAL_SECONDS=1800

# =============================================================================
# Database Configuration
# =============================================================================
DB_PATH=validator.db
USERS_DB_PATH=users.db

# =============================================================================
# API Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Challenge API Configuration
# =============================================================================
CHALLENGE_API_URL=http://localhost:8080
CHALLENGE_API_KEY=your-api-key
CHALLENGE_PUSH_API_KEY=

# =============================================================================
# Evaluation Configuration
# =============================================================================
# Deprecated: now uses CHALLENGE_API_URL
HEATMAP_UPLOAD_URL=http://localhost:8080/heatmap/upload

# =============================================================================
# OpenAI Configuration (for evaluation)
# =============================================================================
OPENAI_API_URL=https://api.openai.com/v1/chat/completions
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4

# =============================================================================
# LLM Configuration (for default model settings)
# =============================================================================
DEFAULT_MODEL=mistralai/Mistral-7B-v0.1
DEFAULT_MAX_TOKENS=512
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=0.95

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO
LOG_FILE=

# =============================================================================
# Test Mode
# =============================================================================
# Enable test mode - picks first response without evaluation or heatmap generation
TEST_MODE=false

# =============================================================================
# Narrative Generation Configuration
# =============================================================================
# Enable narrative generation using LLM. When disabled, evaluation and heatmap
# generation still run, but narrative is skipped (saves LLM API costs).
# Default: true
ENABLE_NARRATIVE_GENERATION=true

# =============================================================================
# Challenge Mode Configuration
# =============================================================================
# Note: Only push mode is supported. Challenges are submitted via POST /challenges endpoint
# CHALLENGE_MODE=push  # Default and only supported mode

# =============================================================================
# Concurrency Configuration
# =============================================================================
# Maximum number of challenges to process concurrently
MAX_CONCURRENT_CHALLENGES=10

# =============================================================================
# Fiber MLTS Configuration
# =============================================================================
# Time-to-live for Fiber symmetric keys in seconds (default: 1 hour)
FIBER_KEY_TTL_SECONDS=3600
# Timeout for Fiber handshake operations in seconds
FIBER_HANDSHAKE_TIMEOUT_SECONDS=30
# Enable automatic key rotation for Fiber symmetric keys
FIBER_ENABLE_KEY_ROTATION=true

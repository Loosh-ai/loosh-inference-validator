version: "1.0.0"

compute_spec:
  validator:
    cpu:
      # Embedding + cosine similarity + small-matrix ops are light; plotting/IO can dominate at high QPS.
      min_cores: 4
      min_speed: 2.5
      recommended_cores: 8
      recommended_speed: 3.5
      architecture: "x86_64"

    gpu:
      # For high-volume / low-latency embeddings, a GPU is strongly recommended.
      required: True

      # Minimum viable for high-volume embeddings (small sentence-transformers models)
      min_vram: 16
      min_compute_capability: 7.5
      minimum_gpu: "NVIDIA T4 16GB (or better)"

      # Recommended sweet spot for sustained high throughput + batching headroom
      recommended_vram: 24
      recommended_compute_capability: 8.6
      recommended_gpu: "NVIDIA A10 24GB"

      # Premium tier for extreme throughput / large batches / multiple embedding models per process
      premium_vram: 80
      premium_compute_capability: 9.0
      premium_gpu: "NVIDIA H100 80GB"

      notes:
        - "For best throughput, batch embeddings across requests (micro-batching)."
        - "Load the SentenceTransformer model once per process (singleton) to avoid per-request load overhead."
        - "Cosine similarity on 3–5 responses is negligible; PNG generation + disk IO can be the bottleneck."

    embeddings:
      # Sentence-transformers embedding workload (no generative inference)
      framework: "sentence-transformers"
      recommended_models:
        - name: "sentence-transformers/all-MiniLM-L6-v2"
          use_case: "fast general-purpose embeddings"
        - name: "sentence-transformers/all-mpnet-base-v2"
          use_case: "higher quality embeddings (more compute)"
      precision:
        # FP16/BF16 are typical on GPU for embeddings; keep FP32 if you need exact reproducibility.
        recommended: "fp16"
      batching:
        # Guidance for high volume; tune based on latency SLOs and text length.
        recommended_microbatch_window_ms: 10
        recommended_batch_size_range: [32, 256]
      evaluation:
        methods:
          - "cosine_similarity"
          - "tf-idf (optional)"
        notes:
          - "Similarity matrix for N=3–5 is tiny; compute cost is dominated by embedding forward pass and plotting/IO."

    memory:
      # GPU embeddings benefit from RAM for queues/batching and for model + tokenizer caches.
      min_ram: 16
      min_swap: 4
      recommended_swap: 8
      ram_type: "DDR5"

    storage:
      # Embedding validators don't need massive disk unless you store lots of plots/artifacts.
      min_space: 200
      recommended_space: 500
      type: "SSD"
      min_iops: 1000
      recommended_iops: 5000
      notes:
        - "If saving heatmaps per request at high QPS, prefer local NVMe and consider sampling or async writes."

    os:
      name: "Ubuntu"
      version: 22.04

    network_spec:
      bandwidth:
        download: 100
        upload: 20
